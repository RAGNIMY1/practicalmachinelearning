---
title: 'ML - Weight Lifting Analysis'
author: "Myriam Ragni - 31 Jan. 2020"
output:
  pdf_document: default
  html_document: default
  word_document: default
  keep_md: yes
  fontsize: 11pt
---

## Executive Summary
The goal of this assignment is to predict the manner in which participants to a Human Activity Recognition study did weight lifting exercises based on data collected by arm/belt/forearm and dumbbell sensors.
Barbell lifts were performed in five different fashions ('classe' variable in the training dataset):   
- Class A: According to the specification (the only correct way of performing barbell lifts)    
- Class B: Throwing the elbows to the front    
- Class C: Lifting the dumbbell only halfway    
- Class D: Lowering the dumbbell only halfway   
- Class E: Throwing the hips to the front   

Thanks to the autors (Ugulino,W.; Cardador,D.; Vega,K.; Velloso,E.; Milidiu, R.; Fuks, H.) of the publication [WearableComputing: Accelerometers' Data Classification of Body Posturesand Movements](http://groupware.les.inf.puc-rio.br/work.jsf?p1=10335) for generously allowing the data collected to be used for this project.   

We are provided with:  
- a 'training' dataset used for the selection of a machine learning algorithm and for validation   
- a 'test' dataset with 20 test cases on which we will apply the chosen model to predict the way the exercice was done.   

I've used the following approach:  
- Based on the reults of the basic exploratory data analysis, I decided to remove data not relevant for the analysis (user information and time stamps, variables with more than 95% of NA or blank values) but to keep near zero variance predictors.    
- For cross-validation purposes, I split further the training data (following the 75/25 pattern) into an explicit training dataset used to train the prediction model and a validation set to evaluate the performance of the selected model.    
- I used Principal Component Analysis to further reduce the dimensionality of data.   
- I chose 3 ML algorithms (Classification Tree, Random Forest and Gradient Boosting Tree) and repeated cross validation with 10 folds and 3 repeats to build the models with the training data and compared the competing models using acuracy as evaluation metric.   
- I picked the model with the best accuracy (Random Forest) and assessed the performance of the RF model with the Validation dataset. The expected accuracy rate (resp. out of sample error) is high enough to validate the model.     
- Finally, the RF model is used to predict the class for the 20 test cases.  


```{r setup, include=FALSE}
knitr::opts_chunk$set(tidy.opts=list(width.cutoff=60), echo = FALSE, eval=TRUE, tidy=TRUE, warning=FALSE, message=FALSE, cache=TRUE, root.dir="c:/RAGNIMY1/datasciencecoursera/MachineLearning")
```

```{r Setenv, results="hide"}
rm(list = ls())
Sys.setlocale("LC_TIME", "English")
suppressWarnings(library(caret))
suppressWarnings(library(doParallel))
```

```{r DownLoadFiles, results="hide"}
SrcFileURL1 <- "http://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
SrcFileURL2  <- "http://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"
pml_training <- "./pml-training.csv"
pml_testing <- "./pml-testing.csv"
#### Check if data files were already downloaded, if not, download the files
if (!file.exists(pml_training)){
                download.file(SrcFileURL1, destfile=pml_training)
}  
if (!file.exists(pml_testing)){
                download.file(SrcFileURL2, destfile=pml_testing)
} 
DF_Train <- read.csv(pml_training, header=TRUE, sep=",") #### raw data training data
DF_Test <- read.csv(pml_testing, header=TRUE, sep=",") #### raw data testing data (for prediction)
```
## Basic Exploratory Data Analysis and Data Cleanup  

```{r Basicinfo, results="hide"}
dim(DF_Train) #### 19622 obs. 160 variables
dim(DF_Test) #### 20 obs. 160 variables
table(DF_Train$classe)
```
```{r TidyDataA, results="hide"}
#### Removing variables containing obvious user information and timestamps
DF_TrainTidy <- DF_Train[,-c(1:7)]
DF_TestTidy <- DF_Test[,-c(1:7)]
```
```{r TidyDataB, results="hide"}
#### Removing predictors with 95% NA or blank values
NACounts <- apply(is.na(DF_Train), MARGIN=2, sum) 
NACounts_Not0 <- NACounts[NACounts != 0] #### All columns with at least one NA --> 67 variables wih 19216 NAs
NAPerc <- sapply(DF_TrainTidy, function(x) mean(is.na(x))| sum(x==""))>0.95 #### Returns TRUE or FALSE
DF_TrainTidy <- DF_TrainTidy[,NAPerc==FALSE] #### 53 predictors left
DF_TestTidy <- DF_TestTidy[,NAPerc==FALSE]
```

The 'training' data set consists of 19622 observations and 160 variables. The first seven variables provide user and time information, which is irrelevant for our problematic and are therefore excluded from the analysis. The analysis also showed an important number of variables with almost only NAs or blank values. Hence, variables with more than 95% of NA or blank values are taken out of the scope. This brings the number of variables in scope to 53.

## Data Partitioning & Preprocessing with PCA 
```{r SplitData, results="hide"}
set.seed(8484)
TrainIndex <- createDataPartition(DF_TrainTidy$classe,p=0.75,list = FALSE) 
Train <- DF_TrainTidy[TrainIndex, ] 
Validate <- DF_TrainTidy[-TrainIndex, ] 
print(paste0("Dimension Training Dataset = ",dim(Train)[1], " ",dim(Train)[2])) #### 14718 obs. 53 variables
print(paste0("Dimension Validation Dataset = ",dim(Validate)[1]," ", dim(Validate)[2])) #### 4904 obs. 53 variables
```

The cleaned training set was split into a pure training dataset (75% of the data) and a validation data set (25%) which will be used to assess the accuracy of the prediction model.    
I also used PCA to further reduce the dimensionality of data, while keeping as much variation as possible. The below plot shows that about 43 components results in variance close to 100%.  

```{r PCA, fig.height = 4, fig.width = 5, results="hide"}
#### Performs PCA, centers the variable to have mean equals to zero, normalizes the variables to have standard deviation equal to 1
prin_comp <- prcomp(Train[,-53], scale. = T) #### exluding the 'classe' variable
names(prin_comp)
std_dev <- prin_comp$sdev
pr_var <- std_dev^2
prop_varex <- pr_var/sum(pr_var)
par(mar = c(4, 5, 1, 2))
plot(cumsum(prop_varex), xlab = "Nr. of Principal Components", ylab = "Cumulative Proportion of Variance", type = "b", col="blue", xaxp = c(0, 53, 53), yaxp = c(0, 1, 10))
grid(nx=20, ny=20, lwd=2, col="grey")
```

Therefore, I selected the components PC1 to PC43 to proceed with the next steps. The same transformation is applied to the Training, Validate and Test datasets. The dimensions of these datasets are respectively:    
```{r DimensionReduction}
#### Set up x and y to avoid slowness of caret() with model syntax
y_Train <- Train[,53]
x_Train <- prin_comp$x[,1:43]
Train <-as.data.frame(cbind(x_Train,y_Train))
#### Transform validate and test datasets into PCA (same transformation to the test set as we did to training set, including the center and scaling feature)
y_Validate <- Validate[,53] #### variable 'classe'
x_Validate <- as.data.frame(predict(prin_comp, newdata = Validate))
x_Validate <- x_Validate[1:43]
Validate <- as.data.frame(cbind(x_Validate,y_Validate))
names(Validate)[44] <- "classe"
Test <- as.data.frame(predict(prin_comp, newdata = DF_TestTidy))
Test <- Test[1:43]
print(paste0("Dimension Training Dataset = ",dim(Train)[1], " ",dim(Train)[2])) #### 14718 obs. 44 variables
print(paste0("Dimension Validate Dataset = ",dim(Validate)[1]," ", dim(Validate)[2])) #### 4904 obs. 44 variables
print(paste0("Dimension Test Dataset = ",dim(Test)[1]," ", dim(Test)[2])) #### 20 obs. 43 variables
```

## Model Selection
```{r Models, results="hide"}
registerDoParallel()
#### Prepare training scheme
Control <- trainControl(method = "cv", number = 5, allowParallel = TRUE)
#### Fit the models
set.seed(8484)
mdl1 <- train(x_Train,y_Train, method="rpart",trControl = Control)
set.seed(8484)
mdl2 <- train(x_Train,y_Train, method="rf",trControl = Control)
set.seed(8484)
mdl3 <- train(x_Train,y_Train, method="gbm",trControl = Control, verbose=FALSE)
``` 

From the 3 competing ML algorithms (Classification Tree, Random Forest and Gradient Boosting Tree), the Random Forest model provides the highest accuracy as shown on the below summary and boxplot.   

```{r Compare}
#### Collect the resampling results
results <- resamples(list(TREE=mdl1, RF=mdl2, GBM=mdl3)) 
summary(results)
```

```{r PlotAccuracy, fig.height = 4, fig.width = 5}
#### Plot the Accuracy metric by model
par(mar = c(2, 2, 1, 2))
bwplot(results, metric = "Accuracy")
```

## Model Assessment (Out of sample error)
The model providing the best accuracy (Random Forest) (with an acceptable level) is now used to verify the performance of the algorith with the Validation dataset, using the confusion matrix.
```{r Validate}
trainpred <- predict(mdl2,newdata=Validate)
confMat <- confusionMatrix(y_Validate,trainpred)
confMat
#### true accuracy of the predicted model 
accuracy <- sum(trainpred == Validate$classe)/length(trainpred)
#### out of sample error (in %)
outOfSampleError <- round((1 - accuracy)*100,2)
```

The calculated expected accuracy rate is **`r round(accuracy*100,2)`%**, which gives an out of sample error rate of **`r outOfSampleError`%**.  

## Prediction
Finally, as the out of sample error rate is acceptable, the selected model is used to predict a class for each of the 20 observations contained in the provided Testing dataset. See results below:

```{r Predict}
testpred <- predict(mdl2,newdata=Test)
testpred
summary(testpred)
```

\newpage
## Appendix 
### Preparing the environment
```{r Setup2, ref.label='Setup', echo=TRUE, eval=TRUE}
```
```{r Setenv2, ref.label='Setenv', echo=TRUE, eval=TRUE, results="hide"}
```
```{r DownLoadFiles2, ref.label='DownLoadFiles', echo=TRUE, eval=TRUE}
```
### Basic Exploratory Data Analysis and Data Cleanup 
```{r Basicinfo2, ref.label='Basicinfo', echo=TRUE, eval=TRUE}
```
```{r TidyDataA2, ref.label='TidyDataA', echo=TRUE, eval=TRUE}
```
```{r TidyDataB2, ref.label='TidyDataB', echo=TRUE, eval=TRUE}
```
### Data Partitioning & Preprocessing with PCA
```{r SplitData2, ref.label='SplitData', echo=TRUE, eval=TRUE}
```
```{r PCA2, ref.label='PCA', echo=TRUE, eval=FALSE}
```
```{r DimensionReduction2, ref.label='DimensionReduction', echo=TRUE, eval=TRUE, results="hide"}
```
### Model Selection
```{r Models2, ref.label='Models', echo=TRUE, eval=TRUE}
```
```{r Compare2, ref.label='Compare', echo=TRUE, eval=TRUE, results="hide"}
```
```{r PlotAccuracy2, ref.label='PlotAccuracy', echo=TRUE, eval=FALSE}
```
### Model Assessment
```{r Validate2, ref.label='Validate', echo=TRUE, eval=TRUE, results="hide"}
```
### Prediction
```{r Predict2, ref.label='Predict', echo=TRUE, eval=TRUE, results="hide"}
```
